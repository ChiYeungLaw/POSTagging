{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS词性标注（BILSTM和BiGRU）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load POS data and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD, UNK = '<pad>', '<unk>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data padding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(sentences, labels, word2idx):\n",
    "    assert len(sentences)==len(labels)\n",
    "    assert np.all([len(sentence)==len(tags) for sentence, tags in zip(sentences, labels)])\n",
    "    max_len = np.max([len(sentence) for sentence in sentences]) # Find out how much to pad\n",
    "    padded_sentences  = list()\n",
    "    padded_labels  = list()\n",
    "    for sentence, tags in zip(sentences, labels):               # Loop over the data\n",
    "        padded_sentence = [word.lower() if word.lower() in word2idx else UNK for word in sentence]  # Sentence, uses the <UNK> symbol for unknown words\n",
    "        padded_tags = [tag for tag in tags]                       # Tags\n",
    "        padded_sentence += [PAD] * max(max_len - len(padded_sentence), 0)\n",
    "        padded_tags += [PAD] * max(max_len - len(padded_tags), 0)\n",
    "        padded_sentences.append(padded_sentence)                  # Append the processed sample to the output\n",
    "        padded_labels.append(padded_tags)\n",
    "    return padded_sentences, padded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(padded_sentences, padded_labels, word2idx, tag2idx, use_cuda):\n",
    "    encoded_sentences = [[word2idx[word] for word in line] for line in padded_sentences]\n",
    "    encoded_labels = [[tag2idx[tag] for tag in line] for line in padded_labels]\n",
    "    if use_cuda:\n",
    "        encoded_sentences = torch.cuda.LongTensor(encoded_sentences)\n",
    "        encoded_labels = torch.cuda.LongTensor(encoded_labels)\n",
    "    else:\n",
    "        encoded_sentences = torch.LongTensor(encoded_sentences)\n",
    "        encoded_labels = torch.LongTensor(encoded_labels)\n",
    "    return encoded_sentences, encoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data batchify function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We input the whole dataset into the `batchify` function and then this function will generate mini-batch data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(word_data, pos_data, batch_size, word2idx, tag2idx, use_cuda):\n",
    "    num_sent = len(word_data)\n",
    "    num_batch = num_sent // batch_size\n",
    "    for i in range(num_batch):\n",
    "        sents, labels = pad_data(word_data[i*batch_size:(i+1)*batch_size], pos_data[i*batch_size:(i+1)*batch_size],\n",
    "                                 word2idx)\n",
    "        yield encode_data(sents, labels, word2idx, tag2idx, use_cuda)\n",
    "    if num_sent % batch_size:\n",
    "        sents, labels = pad_data(word_data[num_batch*batch_size:], pos_data[num_batch*batch_size:],\n",
    "                                 word2idx)\n",
    "        yield encode_data(sents, labels, word2idx, tag2idx, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this function to evaluate our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eva(model, sentences, tags, word2idx, tag2idx, use_cuda):\n",
    "    total, correct = 0, 0\n",
    "    for sent, tag in zip(sentences, tags):\n",
    "        sent = torch.LongTensor([word2idx[word.lower()] if word.lower() in word2idx else word2idx[UNK] for word in sent])\n",
    "        tag = [tag2idx[t] for t in tag]\n",
    "        if use_cuda:\n",
    "            sent = sent.to('cuda')\n",
    "        pred = model.predict(sent)\n",
    "        if use_cuda:\n",
    "            pred = pred.cpu().numpy()\n",
    "        for i in range(len(tag)):\n",
    "            if tag[i] == pred[i]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Chinese POS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UDzhPOS/zh_gsdsimp-ud-train.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "zh_word_data = []\n",
    "zh_pos_data = []\n",
    "word_sent = []\n",
    "pos_sent = []\n",
    "for line in lines:\n",
    "    if '#' not in line and line != '\\n':\n",
    "        line = line.replace('\\t', ' ').replace('\\n', '').split(' ')\n",
    "        word_sent.append(line[1])\n",
    "        pos_sent.append(line[3])\n",
    "    elif \"sent_id\" in line:\n",
    "        zh_word_data.append(word_sent)\n",
    "        zh_pos_data.append(pos_sent)\n",
    "        word_sent, pos_sent = [], []\n",
    "tmp = []\n",
    "for line in zh_word_data:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "zh_word_data = tmp\n",
    "tmp = []\n",
    "for line in zh_pos_data:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "zh_pos_data = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UDzhPOS/zh_gsdsimp-ud-test.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "zh_word_test = []\n",
    "zh_pos_test = []\n",
    "word_sent = []\n",
    "pos_sent = []\n",
    "for line in lines:\n",
    "    if '#' not in line and line != '\\n':\n",
    "        line = line.replace('\\t', ' ').replace('\\n', '').split(' ')\n",
    "        word_sent.append(line[1])\n",
    "        pos_sent.append(line[3])\n",
    "    elif \"sent_id\" in line:\n",
    "        zh_word_test.append(word_sent)\n",
    "        zh_pos_test.append(pos_sent)\n",
    "        word_sent, pos_sent = [], []\n",
    "tmp = []\n",
    "for line in zh_word_test:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "zh_word_test = tmp\n",
    "tmp = []\n",
    "for line in zh_pos_test:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "zh_pos_test = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UDzhPOS/zh_gsdsimp-ud-dev.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "zh_word_dev = []\n",
    "zh_pos_dev = []\n",
    "word_sent = []\n",
    "pos_sent = []\n",
    "for line in lines:\n",
    "    if '#' not in line and line != '\\n':\n",
    "        line = line.replace('\\t', ' ').replace('\\n', '').split(' ')\n",
    "        word_sent.append(line[1])\n",
    "        pos_sent.append(line[3])\n",
    "    elif \"sent_id\" in line:\n",
    "        zh_word_dev.append(word_sent)\n",
    "        zh_pos_dev.append(pos_sent)\n",
    "        word_sent, pos_sent = [], []\n",
    "tmp = []\n",
    "for line in zh_word_dev:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "zh_word_dev = tmp\n",
    "tmp = []\n",
    "for line in zh_pos_dev:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "zh_pos_dev = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the word vocabulary and pos tagging vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_voc = list(set([word for line in word_data for word in line])) + [PAD] + [UNK]\n",
    "zh_pos_voc = list(set([tag for line in pos_data for tag in line])) + [PAD] + [UNK]\n",
    "zh_w2i = {word: idx for idx, word in enumerate(zh_voc)}\n",
    "zh_pos2i = {tag: idx for idx, tag in enumerate(zh_pos_voc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNPOS(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, use_gru,\n",
    "                bidirectional, out_dim, word2idx, dropout_rate=0):\n",
    "        super(RNNPOS, self).__init__()\n",
    "        self.w2i = word2idx\n",
    "        # Embedding layer\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                 embedding_dim=embedding_dim,\n",
    "                                 padding_idx=word2idx[PAD])\n",
    "        self.use_gru = use_gru\n",
    "        if use_gru:\n",
    "            # use_gru = True\n",
    "            # we use gru rnn.\n",
    "            self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size,\n",
    "                             num_layers=num_layers, bidirectional=bidirectional,\n",
    "                             dropout=dropout_rate)\n",
    "        else:\n",
    "            # use_gru = False\n",
    "            # we use LSTM rnn\n",
    "            self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size,\n",
    "                              num_layers=num_layers, bidirectional=bidirectional,\n",
    "                              dropout=dropout_rate)\n",
    "        self.bi = bidirectional\n",
    "        if self.bi:\n",
    "            self.proj = nn.Linear(2 * hidden_size, out_dim)\n",
    "        else:\n",
    "            self.proj = nn.Linear(hidden_size, out_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "    \n",
    "    def forward(self, sentences):\n",
    "        \"\"\"\n",
    "        sentences: [batch_size, max_seq_len]\n",
    "        \"\"\"\n",
    "        embed_x = self.embed(sentences).permute(1, 0, 2) # [max_seq_len, batch_size, embed_dim]\n",
    "        lengths = torch.sum(sentences != self.w2i[PAD], dim=-1) # [batch_size]\n",
    "        pack_x = nn.utils.rnn.pack_padded_sequence(embed_x, lengths, enforce_sorted=False)\n",
    "        pack_output, _ = self.rnn(pack_x)\n",
    "        output = nn.utils.rnn.pad_packed_sequence(pack_output)[0] # [seq_len, batch_size, num_directions * hidden_size]\n",
    "        output = self.proj(self.dropout(output)).permute(1, 0, 2) # [batch_size, seq_len, out_dim]\n",
    "        return output\n",
    "    \n",
    "    def compute_loss(self, sentences, tags):\n",
    "        \"\"\"\n",
    "        We use this function to compute the cross entropy loss.\n",
    "        sentences: [batch_size, max_seq_len]\n",
    "        tags: [batch_size, max_seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = sentences.shape\n",
    "        # we use sent_mask to cover the padding index.\n",
    "        sent_mask = (sentences != self.w2i[PAD]).contiguous().view(batch_size * seq_len, -1) # [batch_size * seq_len, out_dim]\n",
    "        output = self.forward(sentences) # [batch_size, seq_len, out_dim]\n",
    "        output = output.contiguous().view(batch_size * seq_len, -1) # [batch_size * seq_len, out_dim]\n",
    "        loss = nn.functional.cross_entropy(output, tags.view(-1), reduction='none') * sent_mask.to(torch.float32)\n",
    "        return loss.mean()\n",
    "        \n",
    "    def predict(self, sentence):\n",
    "        \"\"\"\n",
    "        We use this function to predict the tag of sentence.\n",
    "        sentence: [seq_len]\n",
    "        \"\"\"\n",
    "        sentence = sentence.unsqueeze(0) # [1, seq_len]\n",
    "        output = self.forward(sentence).squeeze(0) # [seq_len, out_dim]\n",
    "        tag = torch.argmax(output, dim=-1) # [seq_len]\n",
    "        return tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified Chinese POS Data Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(zh_voc)\n",
    "embedding_dim = 100\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "num_epoch = 50\n",
    "batch_size = 32\n",
    "out_dim = len(zh_pos_voc)\n",
    "word2idx = zh_w2i\n",
    "tag2idx = zh_pos2i\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Direction GRU Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gru = True\n",
    "bidirectional = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleGRU = RNNPOS(vocab_size, embedding_dim, hidden_size, num_layers, use_gru,\n",
    "                    bidirectional, out_dim, word2idx, dropout_rate=0).to(device)\n",
    "optimizer = torch.optim.Adam(singleGRU.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50. Total Loss: 123.868. Dev Acc: 55.88%.\n",
      "Epoch: 2/50. Total Loss: 100.481. Dev Acc: 65.85%.\n",
      "Epoch: 3/50. Total Loss: 88.198. Dev Acc: 70.19%.\n",
      "Epoch: 4/50. Total Loss: 78.145. Dev Acc: 72.88%.\n",
      "Epoch: 5/50. Total Loss: 69.236. Dev Acc: 74.80%.\n",
      "Epoch: 6/50. Total Loss: 61.170. Dev Acc: 76.15%.\n",
      "Epoch: 7/50. Total Loss: 53.829. Dev Acc: 76.94%.\n",
      "Epoch: 8/50. Total Loss: 47.185. Dev Acc: 77.46%.\n",
      "Epoch: 9/50. Total Loss: 41.242. Dev Acc: 77.80%.\n",
      "Early Stop!\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0.\n",
    "    for sentences, tags in batchify(zh_word_data, zh_pos_data, batch_size, word2idx, tag2idx, use_cuda):\n",
    "        optimizer.zero_grad()\n",
    "        loss = singleGRU.compute_loss(sentences, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    dev_acc = eva(singleGRU, zh_word_dev, zh_pos_dev, zh_w2i, zh_pos2i, use_cuda)\n",
    "    if dev_acc >= best_acc:\n",
    "        best_acc = dev_acc\n",
    "        torch.save(singleGRU.state_dict(), 'singleGRU.pt')\n",
    "        print(f\"Epoch: {epoch+1}/{num_epoch}. Total Loss: {total_loss:.3f}. Dev Acc: {dev_acc:.2%}.\")\n",
    "    else:\n",
    "        print(\"Early Stop!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 77.567%\n"
     ]
    }
   ],
   "source": [
    "test_acc = eva(singleGRU, zh_word_test, zh_pos_test, zh_w2i, zh_pos2i, use_cuda)\n",
    "print(f\"Test Set Accuracy: {test_acc:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional GRU Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gru = True\n",
    "bidirectional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "biGRU = RNNPOS(vocab_size, embedding_dim, hidden_size, num_layers, use_gru,\n",
    "               bidirectional, out_dim, word2idx, dropout_rate=0).to(device)\n",
    "optimizer = torch.optim.Adam(biGRU.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50. Total Loss: 114.259. Dev Acc: 62.17%.\n",
      "Epoch: 2/50. Total Loss: 92.377. Dev Acc: 69.81%.\n",
      "Epoch: 3/50. Total Loss: 80.607. Dev Acc: 73.72%.\n",
      "Epoch: 4/50. Total Loss: 70.808. Dev Acc: 75.65%.\n",
      "Epoch: 5/50. Total Loss: 62.142. Dev Acc: 76.94%.\n",
      "Epoch: 6/50. Total Loss: 54.371. Dev Acc: 77.58%.\n",
      "Epoch: 7/50. Total Loss: 47.460. Dev Acc: 77.95%.\n",
      "Epoch: 8/50. Total Loss: 41.427. Dev Acc: 78.01%.\n",
      "Early Stop!\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0.\n",
    "    for sentences, tags in batchify(zh_word_data, zh_pos_data, batch_size, word2idx, tag2idx, use_cuda):\n",
    "        optimizer.zero_grad()\n",
    "        loss = biGRU.compute_loss(sentences, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    dev_acc = eva(biGRU, zh_word_dev, zh_pos_dev, zh_w2i, zh_pos2i, use_cuda)\n",
    "    if dev_acc >= best_acc:\n",
    "        best_acc = dev_acc\n",
    "        torch.save(biGRU.state_dict(), 'biGRU.pt')\n",
    "        print(f\"Epoch: {epoch+1}/{num_epoch}. Total Loss: {total_loss:.3f}. Dev Acc: {dev_acc:.2%}.\")\n",
    "    else:\n",
    "        print(\"Early Stop!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 78.117%\n"
     ]
    }
   ],
   "source": [
    "test_acc = eva(biGRU, zh_word_test, zh_pos_test, zh_w2i, zh_pos2i, use_cuda)\n",
    "print(f\"Test Set Accuracy: {test_acc:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Direction LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gru = False\n",
    "bidirectional = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleLSTM = RNNPOS(vocab_size, embedding_dim, hidden_size, num_layers, use_gru,\n",
    "                    bidirectional, out_dim, word2idx, dropout_rate=0).to(device)\n",
    "optimizer = torch.optim.Adam(singleLSTM.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50. Total Loss: 121.429. Dev Acc: 55.48%.\n",
      "Epoch: 2/50. Total Loss: 98.156. Dev Acc: 65.75%.\n",
      "Epoch: 3/50. Total Loss: 85.749. Dev Acc: 70.64%.\n",
      "Epoch: 4/50. Total Loss: 75.736. Dev Acc: 73.52%.\n",
      "Epoch: 5/50. Total Loss: 66.936. Dev Acc: 75.31%.\n",
      "Epoch: 6/50. Total Loss: 59.000. Dev Acc: 76.49%.\n",
      "Epoch: 7/50. Total Loss: 51.806. Dev Acc: 77.26%.\n",
      "Epoch: 8/50. Total Loss: 45.319. Dev Acc: 77.75%.\n",
      "Epoch: 9/50. Total Loss: 39.530. Dev Acc: 77.78%.\n",
      "Early Stop!\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0.\n",
    "    for sentences, tags in batchify(zh_word_data, zh_pos_data, batch_size, word2idx, tag2idx, use_cuda):\n",
    "        optimizer.zero_grad()\n",
    "        loss = singleLSTM.compute_loss(sentences, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    dev_acc = eva(singleLSTM, zh_word_dev, zh_pos_dev, zh_w2i, zh_pos2i, use_cuda)\n",
    "    if dev_acc >= best_acc:\n",
    "        best_acc = dev_acc\n",
    "        torch.save(singleLSTM.state_dict(), 'singleLSTM.pt')\n",
    "        print(f\"Epoch: {epoch+1}/{num_epoch}. Total Loss: {total_loss:.3f}. Dev Acc: {dev_acc:.2%}.\")\n",
    "    else:\n",
    "        print(\"Early Stop!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 77.458%\n"
     ]
    }
   ],
   "source": [
    "test_acc = eva(singleLSTM, zh_word_test, zh_pos_test, zh_w2i, zh_pos2i, use_cuda)\n",
    "print(f\"Test Set Accuracy: {test_acc:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gru = False\n",
    "bidirectional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "biLSTM = RNNPOS(vocab_size, embedding_dim, hidden_size, num_layers, use_gru,\n",
    "               bidirectional, out_dim, word2idx, dropout_rate=0).to(device)\n",
    "optimizer = torch.optim.Adam(biLSTM.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50. Total Loss: 118.636. Dev Acc: 57.82%.\n",
      "Epoch: 2/50. Total Loss: 93.972. Dev Acc: 68.43%.\n",
      "Epoch: 3/50. Total Loss: 81.003. Dev Acc: 73.35%.\n",
      "Epoch: 4/50. Total Loss: 70.841. Dev Acc: 75.77%.\n",
      "Epoch: 5/50. Total Loss: 62.059. Dev Acc: 77.21%.\n",
      "Epoch: 6/50. Total Loss: 54.311. Dev Acc: 78.02%.\n",
      "Epoch: 7/50. Total Loss: 47.581. Dev Acc: 78.07%.\n",
      "Epoch: 8/50. Total Loss: 41.453. Dev Acc: 78.26%.\n",
      "Epoch: 9/50. Total Loss: 36.127. Dev Acc: 78.93%.\n",
      "Early Stop!\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0.\n",
    "    for sentences, tags in batchify(zh_word_data, zh_pos_data, batch_size, word2idx, tag2idx, use_cuda):\n",
    "        optimizer.zero_grad()\n",
    "        loss = biLSTM.compute_loss(sentences, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    dev_acc = eva(biLSTM, zh_word_dev, zh_pos_dev, zh_w2i, zh_pos2i, use_cuda)\n",
    "    if dev_acc >= best_acc:\n",
    "        best_acc = dev_acc\n",
    "        torch.save(biLSTM.state_dict(), 'biLSTM.pt')\n",
    "        print(f\"Epoch: {epoch+1}/{num_epoch}. Total Loss: {total_loss:.3f}. Dev Acc: {dev_acc:.2%}.\")\n",
    "    else:\n",
    "        print(\"Early Stop!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 79.043%\n"
     ]
    }
   ],
   "source": [
    "test_acc = eva(biLSTM, zh_word_test, zh_pos_test, zh_w2i, zh_pos2i, use_cuda)\n",
    "print(f\"Test Set Accuracy: {test_acc:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparasion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 3.2~3.5, we train 4 different models, including single direction GRU, bidirectional GRU, single direction LSTM and bidirectional LSTM. From the table below, we can find that bidirectional LSTM model is the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>singleGRU</th>\n",
       "      <td>0.77567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biGRU</th>\n",
       "      <td>0.78117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>singleLSTM</th>\n",
       "      <td>0.77458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biLSTM</th>\n",
       "      <td>0.79043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            test acc\n",
       "singleGRU    0.77567\n",
       "biGRU        0.78117\n",
       "singleLSTM   0.77458\n",
       "biLSTM       0.79043"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame([0.77567, 0.78117, 0.77458, 0.79043], index = ['singleGRU', 'biGRU', 'singleLSTM', 'biLSTM'], columns=['test acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use biLSTM model with different dropout value to find the effect of dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gru = False\n",
    "bidirectional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Training!\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin Training!\")\n",
    "dropout_rate = [0, 0.2, 0.4, 0.6, 0.8]\n",
    "test_acc_list = []\n",
    "for dp in dropout_rate:\n",
    "    biLSTM = RNNPOS(vocab_size, embedding_dim, hidden_size, num_layers, use_gru,\n",
    "               bidirectional, out_dim, word2idx, dropout_rate=dp).to(device)\n",
    "    optimizer = torch.optim.Adam(biLSTM.parameters(), lr=learning_rate)\n",
    "    # Model Training\n",
    "    best_acc = 0.\n",
    "    for epoch in range(num_epoch):\n",
    "        total_loss = 0.\n",
    "        for sentences, tags in batchify(zh_word_data, zh_pos_data, batch_size, word2idx, tag2idx, use_cuda):\n",
    "            optimizer.zero_grad()\n",
    "            loss = biLSTM.compute_loss(sentences, tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        dev_acc = eva(biLSTM, zh_word_dev, zh_pos_dev, zh_w2i, zh_pos2i, use_cuda)\n",
    "        if dev_acc >= best_acc:\n",
    "            best_acc = dev_acc\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Model evaluation\n",
    "    test_acc = eva(biLSTM, zh_word_test, zh_pos_test, zh_w2i, zh_pos2i, use_cuda)\n",
    "    test_acc_list.append(test_acc)\n",
    "print(\"Finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dropout_rate, test_acc_list, marker='s')\n",
    "plt.xlabel('dropout rate')\n",
    "plt.ylabel('test acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can find that when `dropout_rate = 0.2`, biLSTM model has the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Language POS Data Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the biLSTM model in 4 other different languages, including English, Japanese, Korean and Portuguese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English POS Data Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English POS Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UDenPOS/en_ewt-ud-train.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "en_word_data = []\n",
    "en_pos_data = []\n",
    "word_sent = []\n",
    "pos_sent = []\n",
    "for line in lines:\n",
    "    if '#' not in line and line != '\\n':\n",
    "        line = line.replace('\\t', ' ').replace('\\n', '').split(' ')\n",
    "        word_sent.append(line[1])\n",
    "        pos_sent.append(line[3])\n",
    "    elif \"sent_id\" in line:\n",
    "        en_word_data.append(word_sent)\n",
    "        en_pos_data.append(pos_sent)\n",
    "        word_sent, pos_sent = [], []\n",
    "tmp = []\n",
    "for line in en_word_data:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "en_word_data = tmp\n",
    "tmp = []\n",
    "for line in en_pos_data:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "en_pos_data = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UDenPOS/en_ewt-ud-test.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "en_word_test = []\n",
    "en_pos_test = []\n",
    "word_sent = []\n",
    "pos_sent = []\n",
    "for line in lines:\n",
    "    if '#' not in line and line != '\\n':\n",
    "        line = line.replace('\\t', ' ').replace('\\n', '').split(' ')\n",
    "        word_sent.append(line[1])\n",
    "        pos_sent.append(line[3])\n",
    "    elif \"sent_id\" in line:\n",
    "        en_word_test.append(word_sent)\n",
    "        en_pos_test.append(pos_sent)\n",
    "        word_sent, pos_sent = [], []\n",
    "tmp = []\n",
    "for line in en_word_test:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "en_word_test = tmp\n",
    "tmp = []\n",
    "for line in en_pos_test:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "en_pos_test = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UDenPOS/en_ewt-ud-dev.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "en_word_dev = []\n",
    "en_pos_dev = []\n",
    "word_sent = []\n",
    "pos_sent = []\n",
    "for line in lines:\n",
    "    if '#' not in line and line != '\\n':\n",
    "        line = line.replace('\\t', ' ').replace('\\n', '').split(' ')\n",
    "        word_sent.append(line[1])\n",
    "        pos_sent.append(line[3])\n",
    "    elif \"sent_id\" in line:\n",
    "        en_word_dev.append(word_sent)\n",
    "        en_pos_dev.append(pos_sent)\n",
    "        word_sent, pos_sent = [], []\n",
    "tmp = []\n",
    "for line in en_word_dev:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "en_word_dev = tmp\n",
    "tmp = []\n",
    "for line in en_pos_dev:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "en_pos_dev = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_voc = list(set([word for line in en_word_data for word in line])) + [PAD] + [UNK]\n",
    "en_pos_voc = list(set([tag for line in en_pos_data for tag in line])) + [PAD] + [UNK]\n",
    "en_w2i = {word: idx for idx, word in enumerate(en_voc)}\n",
    "en_pos2i = {tag: idx for idx, tag in enumerate(en_pos_voc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "embedding_dim = 100\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "num_epoch = 50\n",
    "batch_size = 32\n",
    "use_cuda = True\n",
    "word2idx = en_w2i\n",
    "tag2idx = en_pos2i\n",
    "vocab_size = len(en_voc)\n",
    "out_dim = len(en_pos_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50. Total Loss: 275.552. Dev Acc: 79.83%.\n",
      "Epoch: 2/50. Total Loss: 185.352. Dev Acc: 84.63%.\n",
      "Epoch: 3/50. Total Loss: 130.085. Dev Acc: 86.60%.\n",
      "Epoch: 4/50. Total Loss: 89.610. Dev Acc: 87.30%.\n",
      "Epoch: 5/50. Total Loss: 61.671. Dev Acc: 87.60%.\n",
      "Epoch: 6/50. Total Loss: 43.304. Dev Acc: 87.93%.\n",
      "Epoch: 7/50. Total Loss: 31.246. Dev Acc: 88.26%.\n",
      "Epoch: 8/50. Total Loss: 23.186. Dev Acc: 88.39%.\n",
      "Epoch: 9/50. Total Loss: 17.546. Dev Acc: 88.81%.\n",
      "Early Stop!\n"
     ]
    }
   ],
   "source": [
    "biLSTM = RNNPOS(vocab_size, embedding_dim, hidden_size, num_layers, False,\n",
    "               True, out_dim, word2idx, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(biLSTM.parameters(), lr=learning_rate)\n",
    "best_acc = 0\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0.\n",
    "    for sentences, tags in batchify(en_word_data, en_pos_data, batch_size, word2idx, tag2idx, use_cuda):\n",
    "        optimizer.zero_grad()\n",
    "        loss = biLSTM.compute_loss(sentences, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    dev_acc = eva(biLSTM, en_word_dev, en_pos_dev, en_w2i, en_pos2i, use_cuda)\n",
    "    if dev_acc >= best_acc:\n",
    "        best_acc = dev_acc\n",
    "        torch.save(biLSTM.state_dict(), 'enbiLSTM.pt')\n",
    "        print(f\"Epoch: {epoch+1}/{num_epoch}. Total Loss: {total_loss:.3f}. Dev Acc: {dev_acc:.2%}.\")\n",
    "    else:\n",
    "        print(\"Early Stop!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 89.013%\n"
     ]
    }
   ],
   "source": [
    "test_acc = eva(biLSTM, en_word_test, en_pos_test, en_w2i, en_pos2i, use_cuda)\n",
    "print(f\"Test Set Accuracy: {test_acc:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Japanese POS Data Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Japanese POS Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UDjaPOS/ja_gsd-ud-train.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "ja_word_data = []\n",
    "ja_pos_data = []\n",
    "word_sent = []\n",
    "pos_sent = []\n",
    "for line in lines:\n",
    "    if '#' not in line and line != '\\n':\n",
    "        line = line.replace('\\t', ' ').replace('\\n', '').split(' ')\n",
    "        word_sent.append(line[1])\n",
    "        pos_sent.append(line[3])\n",
    "    elif \"sent_id\" in line:\n",
    "        ja_word_data.append(word_sent)\n",
    "        ja_pos_data.append(pos_sent)\n",
    "        word_sent, pos_sent = [], []\n",
    "tmp = []\n",
    "for line in ja_word_data:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "ja_word_data = tmp\n",
    "tmp = []\n",
    "for line in ja_pos_data:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "ja_pos_data = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UDjaPOS/ja_gsd-ud-test.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "ja_word_test = []\n",
    "ja_pos_test = []\n",
    "word_sent = []\n",
    "pos_sent = []\n",
    "for line in lines:\n",
    "    if '#' not in line and line != '\\n':\n",
    "        line = line.replace('\\t', ' ').replace('\\n', '').split(' ')\n",
    "        word_sent.append(line[1])\n",
    "        pos_sent.append(line[3])\n",
    "    elif \"sent_id\" in line:\n",
    "        ja_word_test.append(word_sent)\n",
    "        ja_pos_test.append(pos_sent)\n",
    "        word_sent, pos_sent = [], []\n",
    "tmp = []\n",
    "for line in ja_word_test:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "ja_word_test = tmp\n",
    "tmp = []\n",
    "for line in ja_pos_test:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "ja_pos_test = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UDjaPOS/ja_gsd-ud-dev.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "ja_word_dev = []\n",
    "ja_pos_dev = []\n",
    "word_sent = []\n",
    "pos_sent = []\n",
    "for line in lines:\n",
    "    if '#' not in line and line != '\\n':\n",
    "        line = line.replace('\\t', ' ').replace('\\n', '').split(' ')\n",
    "        word_sent.append(line[1])\n",
    "        pos_sent.append(line[3])\n",
    "    elif \"sent_id\" in line:\n",
    "        ja_word_dev.append(word_sent)\n",
    "        ja_pos_dev.append(pos_sent)\n",
    "        word_sent, pos_sent = [], []\n",
    "tmp = []\n",
    "for line in ja_word_dev:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "ja_word_dev = tmp\n",
    "tmp = []\n",
    "for line in ja_pos_dev:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "ja_pos_dev = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja_voc = list(set([word for line in ja_word_data for word in line])) + [PAD] + [UNK]\n",
    "ja_pos_voc = list(set([tag for line in ja_pos_data for tag in line])) + [PAD] + [UNK]\n",
    "ja_w2i = {word: idx for idx, word in enumerate(ja_voc)}\n",
    "ja_pos2i = {tag: idx for idx, tag in enumerate(ja_pos_voc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "embedding_dim = 100\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "num_epoch = 50\n",
    "batch_size = 32\n",
    "use_cuda = True\n",
    "word2idx = ja_w2i\n",
    "tag2idx = ja_pos2i\n",
    "vocab_size = len(ja_voc)\n",
    "out_dim = len(ja_pos_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50. Total Loss: 175.774. Dev Acc: 83.98%.\n",
      "Epoch: 2/50. Total Loss: 134.665. Dev Acc: 87.38%.\n",
      "Epoch: 3/50. Total Loss: 111.396. Dev Acc: 88.50%.\n",
      "Epoch: 4/50. Total Loss: 91.170. Dev Acc: 89.55%.\n",
      "Epoch: 5/50. Total Loss: 73.588. Dev Acc: 89.84%.\n",
      "Epoch: 6/50. Total Loss: 58.703. Dev Acc: 90.42%.\n",
      "Epoch: 7/50. Total Loss: 46.596. Dev Acc: 90.79%.\n",
      "Epoch: 8/50. Total Loss: 37.010. Dev Acc: 91.29%.\n",
      "Epoch: 9/50. Total Loss: 29.481. Dev Acc: 91.56%.\n",
      "Epoch: 10/50. Total Loss: 23.634. Dev Acc: 91.96%.\n",
      "Epoch: 11/50. Total Loss: 19.155. Dev Acc: 92.23%.\n",
      "Early Stop!\n"
     ]
    }
   ],
   "source": [
    "biLSTM = RNNPOS(vocab_size, embedding_dim, hidden_size, num_layers, False,\n",
    "               True, out_dim, word2idx, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(biLSTM.parameters(), lr=learning_rate)\n",
    "best_acc = 0\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0.\n",
    "    for sentences, tags in batchify(ja_word_data, ja_pos_data, batch_size, word2idx, tag2idx, use_cuda):\n",
    "        optimizer.zero_grad()\n",
    "        loss = biLSTM.compute_loss(sentences, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    dev_acc = eva(biLSTM, ja_word_dev, ja_pos_dev, ja_w2i, ja_pos2i, use_cuda)\n",
    "    if dev_acc >= best_acc:\n",
    "        best_acc = dev_acc\n",
    "        torch.save(biLSTM.state_dict(), 'jabiLSTM.pt')\n",
    "        print(f\"Epoch: {epoch+1}/{num_epoch}. Total Loss: {total_loss:.3f}. Dev Acc: {dev_acc:.2%}.\")\n",
    "    else:\n",
    "        print(\"Early Stop!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 90.895%\n"
     ]
    }
   ],
   "source": [
    "test_acc = eva(biLSTM, ja_word_test, ja_pos_test, ja_w2i, ja_pos2i, use_cuda)\n",
    "print(f\"Test Set Accuracy: {test_acc:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Korean POS Data Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korean POS Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UDkoPOS/ko_gsd-ud-train.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "ko_word_data = []\n",
    "ko_pos_data = []\n",
    "word_sent = []\n",
    "pos_sent = []\n",
    "for line in lines:\n",
    "    if '#' not in line and line != '\\n':\n",
    "        line = line.replace('\\t', ' ').replace('\\n', '').split(' ')\n",
    "        word_sent.append(line[1])\n",
    "        pos_sent.append(line[3])\n",
    "    elif \"sent_id\" in line:\n",
    "        ko_word_data.append(word_sent)\n",
    "        ko_pos_data.append(pos_sent)\n",
    "        word_sent, pos_sent = [], []\n",
    "tmp = []\n",
    "for line in ko_word_data:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "ko_word_data = tmp\n",
    "tmp = []\n",
    "for line in ko_pos_data:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "ko_pos_data = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UDkoPOS/ko_gsd-ud-test.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "ko_word_test = []\n",
    "ko_pos_test = []\n",
    "word_sent = []\n",
    "pos_sent = []\n",
    "for line in lines:\n",
    "    if '#' not in line and line != '\\n':\n",
    "        line = line.replace('\\t', ' ').replace('\\n', '').split(' ')\n",
    "        word_sent.append(line[1])\n",
    "        pos_sent.append(line[3])\n",
    "    elif \"sent_id\" in line:\n",
    "        ko_word_test.append(word_sent)\n",
    "        ko_pos_test.append(pos_sent)\n",
    "        word_sent, pos_sent = [], []\n",
    "tmp = []\n",
    "for line in ko_word_test:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "ko_word_test = tmp\n",
    "tmp = []\n",
    "for line in ko_pos_test:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "ko_pos_test = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UDkoPOS/ko_gsd-ud-dev.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "ko_word_dev = []\n",
    "ko_pos_dev = []\n",
    "word_sent = []\n",
    "pos_sent = []\n",
    "for line in lines:\n",
    "    if '#' not in line and line != '\\n':\n",
    "        line = line.replace('\\t', ' ').replace('\\n', '').split(' ')\n",
    "        word_sent.append(line[1])\n",
    "        pos_sent.append(line[3])\n",
    "    elif \"sent_id\" in line:\n",
    "        ko_word_dev.append(word_sent)\n",
    "        ko_pos_dev.append(pos_sent)\n",
    "        word_sent, pos_sent = [], []\n",
    "tmp = []\n",
    "for line in ko_word_dev:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "ko_word_dev = tmp\n",
    "tmp = []\n",
    "for line in ko_pos_dev:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "ko_pos_dev = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_voc = list(set([word for line in ko_word_data for word in line])) + [PAD] + [UNK]\n",
    "ko_pos_voc = list(set([tag for line in ko_pos_data for tag in line])) + [PAD] + [UNK]\n",
    "ko_w2i = {word: idx for idx, word in enumerate(ko_voc)}\n",
    "ko_pos2i = {tag: idx for idx, tag in enumerate(ko_pos_voc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "embedding_dim = 100\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "num_epoch = 50\n",
    "batch_size = 32\n",
    "use_cuda = True\n",
    "word2idx = ko_w2i\n",
    "tag2idx = ko_pos2i\n",
    "vocab_size = len(ko_voc)\n",
    "out_dim = len(ko_pos_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50. Total Loss: 110.084. Dev Acc: 58.53%.\n",
      "Epoch: 2/50. Total Loss: 95.124. Dev Acc: 62.50%.\n",
      "Epoch: 3/50. Total Loss: 84.601. Dev Acc: 65.94%.\n",
      "Epoch: 4/50. Total Loss: 74.557. Dev Acc: 69.60%.\n",
      "Epoch: 5/50. Total Loss: 65.203. Dev Acc: 71.54%.\n",
      "Epoch: 6/50. Total Loss: 56.553. Dev Acc: 72.01%.\n",
      "Epoch: 7/50. Total Loss: 48.740. Dev Acc: 73.78%.\n",
      "Epoch: 8/50. Total Loss: 42.114. Dev Acc: 74.54%.\n",
      "Early Stop!\n"
     ]
    }
   ],
   "source": [
    "biLSTM = RNNPOS(vocab_size, embedding_dim, hidden_size, num_layers, False,\n",
    "               True, out_dim, word2idx, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(biLSTM.parameters(), lr=learning_rate)\n",
    "best_acc = 0\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0.\n",
    "    for sentences, tags in batchify(ko_word_data, ko_pos_data, batch_size, word2idx, tag2idx, use_cuda):\n",
    "        optimizer.zero_grad()\n",
    "        loss = biLSTM.compute_loss(sentences, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    dev_acc = eva(biLSTM, ko_word_dev, ko_pos_dev, ko_w2i, ko_pos2i, use_cuda)\n",
    "    if dev_acc >= best_acc:\n",
    "        best_acc = dev_acc\n",
    "        torch.save(biLSTM.state_dict(), 'kobiLSTM.pt')\n",
    "        print(f\"Epoch: {epoch+1}/{num_epoch}. Total Loss: {total_loss:.3f}. Dev Acc: {dev_acc:.2%}.\")\n",
    "    else:\n",
    "        print(\"Early Stop!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 76.380%\n"
     ]
    }
   ],
   "source": [
    "test_acc = eva(biLSTM, ko_word_test, ko_pos_test, ko_w2i, ko_pos2i, use_cuda)\n",
    "print(f\"Test Set Accuracy: {test_acc:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portuguese POS Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Portuguese POS Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UDptPOS/pt_gsd-ud-train.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "pt_word_data = []\n",
    "pt_pos_data = []\n",
    "word_sent = []\n",
    "pos_sent = []\n",
    "for line in lines:\n",
    "    if '#' not in line and line != '\\n':\n",
    "        line = line.replace('\\t', ' ').replace('\\n', '').split(' ')\n",
    "        word_sent.append(line[1])\n",
    "        pos_sent.append(line[3])\n",
    "    elif \"sent_id\" in line:\n",
    "        pt_word_data.append(word_sent)\n",
    "        pt_pos_data.append(pos_sent)\n",
    "        word_sent, pos_sent = [], []\n",
    "tmp = []\n",
    "for line in pt_word_data:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "pt_word_data = tmp\n",
    "tmp = []\n",
    "for line in pt_pos_data:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "pt_pos_data = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UDptPOS/pt_gsd-ud-test.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "pt_word_test = []\n",
    "pt_pos_test = []\n",
    "word_sent = []\n",
    "pos_sent = []\n",
    "for line in lines:\n",
    "    if '#' not in line and line != '\\n':\n",
    "        line = line.replace('\\t', ' ').replace('\\n', '').split(' ')\n",
    "        word_sent.append(line[1])\n",
    "        pos_sent.append(line[3])\n",
    "    elif \"sent_id\" in line:\n",
    "        pt_word_test.append(word_sent)\n",
    "        pt_pos_test.append(pos_sent)\n",
    "        word_sent, pos_sent = [], []\n",
    "tmp = []\n",
    "for line in pt_word_test:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "pt_word_test = tmp\n",
    "tmp = []\n",
    "for line in pt_pos_test:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "pt_pos_test = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UDptPOS/pt_gsd-ud-dev.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "pt_word_dev = []\n",
    "pt_pos_dev = []\n",
    "word_sent = []\n",
    "pos_sent = []\n",
    "for line in lines:\n",
    "    if '#' not in line and line != '\\n':\n",
    "        line = line.replace('\\t', ' ').replace('\\n', '').split(' ')\n",
    "        word_sent.append(line[1])\n",
    "        pos_sent.append(line[3])\n",
    "    elif \"sent_id\" in line:\n",
    "        pt_word_dev.append(word_sent)\n",
    "        pt_pos_dev.append(pos_sent)\n",
    "        word_sent, pos_sent = [], []\n",
    "tmp = []\n",
    "for line in pt_word_dev:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "pt_word_dev = tmp\n",
    "tmp = []\n",
    "for line in pt_pos_dev:\n",
    "    if line != []:\n",
    "        tmp.append(line)\n",
    "pt_pos_dev = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_voc = list(set([word for line in pt_word_data for word in line])) + [PAD] + [UNK]\n",
    "pt_pos_voc = list(set([tag for line in pt_pos_data for tag in line])) + [PAD] + [UNK]\n",
    "pt_w2i = {word: idx for idx, word in enumerate(pt_voc)}\n",
    "pt_pos2i = {tag: idx for idx, tag in enumerate(pt_pos_voc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "embedding_dim = 100\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "num_epoch = 50\n",
    "batch_size = 32\n",
    "use_cuda = True\n",
    "word2idx = pt_w2i\n",
    "tag2idx = pt_pos2i\n",
    "vocab_size = len(pt_voc)\n",
    "out_dim = len(pt_pos_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50. Total Loss: 235.111. Dev Acc: 83.69%.\n",
      "Epoch: 2/50. Total Loss: 171.075. Dev Acc: 87.62%.\n",
      "Epoch: 3/50. Total Loss: 131.433. Dev Acc: 89.16%.\n",
      "Epoch: 4/50. Total Loss: 99.220. Dev Acc: 90.07%.\n",
      "Epoch: 5/50. Total Loss: 73.962. Dev Acc: 90.39%.\n",
      "Epoch: 6/50. Total Loss: 55.051. Dev Acc: 90.62%.\n",
      "Epoch: 7/50. Total Loss: 41.309. Dev Acc: 90.67%.\n",
      "Early Stop!\n"
     ]
    }
   ],
   "source": [
    "biLSTM = RNNPOS(vocab_size, embedding_dim, hidden_size, num_layers, False,\n",
    "               True, out_dim, word2idx, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(biLSTM.parameters(), lr=learning_rate)\n",
    "best_acc = 0\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0.\n",
    "    for sentences, tags in batchify(pt_word_data, pt_pos_data, batch_size, word2idx, tag2idx, use_cuda):\n",
    "        optimizer.zero_grad()\n",
    "        loss = biLSTM.compute_loss(sentences, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    dev_acc = eva(biLSTM, pt_word_dev, pt_pos_dev, pt_w2i, pt_pos2i, use_cuda)\n",
    "    if dev_acc >= best_acc:\n",
    "        best_acc = dev_acc\n",
    "        torch.save(biLSTM.state_dict(), 'ptbiLSTM.pt')\n",
    "        print(f\"Epoch: {epoch+1}/{num_epoch}. Total Loss: {total_loss:.3f}. Dev Acc: {dev_acc:.2%}.\")\n",
    "    else:\n",
    "        print(\"Early Stop!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 90.099%\n"
     ]
    }
   ],
   "source": [
    "test_acc = eva(biLSTM, pt_word_test, pt_pos_test, pt_w2i, pt_pos2i, use_cuda)\n",
    "print(f\"Test Set Accuracy: {test_acc:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Languages Comparasion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From section, we can find that the accuracies are different for different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>simplified Chinese</th>\n",
       "      <td>0.78651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>0.89013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>0.90895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>0.76380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portuguese</th>\n",
       "      <td>0.90099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Test acc\n",
       "simplified Chinese   0.78651\n",
       "English              0.89013\n",
       "Japanese             0.90895\n",
       "Korean               0.76380\n",
       "Portuguese           0.90099"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=[0.78651, 0.89013, 0.90895, 0.76380, 0.90099],\n",
    "            index=['simplified Chinese', 'English', 'Japanese', 'Korean', 'Portuguese'],\n",
    "            columns=['Test acc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
